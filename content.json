{"pages":[{"title":"","text":"介绍硕士/江西财经大学/信息管理学院/计算机技术专业Github: https://github.com/guiyanakuangEmail: guiyanakuang@gmail.com 技能清单开发语言：C/C++/Java/JavaScript/PHP/Python/Scala前端开发：Vue移动开发：Android数据相关：Lucene/MySQL/Hadoop/Spark开发环境：Windows/Linux版本管理、文档和自动化部署工具：Git","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"Merry Christmas！","text":"2016年，从学校走向社会，生活节奏自由而明快。告别实验室到寝室的两点一线，拥抱大广州的温暖明媚。 一、二月苦逼的论文季。15年找到工作后便拖延症附体，只得寒假里闭关，记得把自己关在家里足足有一个月，整日与office相依为命，孤苦愤懑，好歹是把初稿给挤了出来（六月份又被一通魔改那就是后话了），精神消瘦，肉体膨胀。 《深入理解计算机系统》 这是这两个月看的唯一一本技术书，简直圣经级教材，全面的的讲解了计算机整体的运行机理，完全弥补了我本科组成原理、操作系统神游四方的知识空洞。我就纳闷当年怎么不用此书开课，非得啃国内老教授晦涩难懂，一直纠结硬件层，让人晕头转向的大作。 三、四、五月新鲜的实习季。初来数据运营组实习，我的工作任务主要是接手一个PHP的Web工程，还好PHP是世界上最好的语言，花了一个星期的时间从入门到放弃上手。但最最痛苦的是写Hive SQL，1.3版本的Hive慢到乌龟爬，Bug到处有，对于急性子简直是天生的折磨（还好我战胜了它 o(╬￣皿￣)=○# (￣#)3￣)）。这段时间里最大的收获倒不是学习了新的语言，新的查询工具，而是认识到数据组整体的工作本质。一开始使用上报SDK进行数据埋点，消息队列、Hadoop对再对数据进行收集清洗，定时调度生成多维度的数据报表提供给数据分析人员，帮助运营人员决策。 《深入PHP面向对象、模式与实践》、《Modern PHP》 PHP入门我看的是这两本书，如果对面向对象、设计模式比较熟悉，看完语法层第一本书就可以跳着过一遍了，第二本则侧重实践风格、工具使用、测试规范等等。《Hive编程指南》 一本工具书，不懂就翻，相对来说Hive资料网上相对较少，这是一个很好的补充。 六月烦闷的答辩季。一心想成为小透明的我，还是应验了墨菲定律，论文抽中万五。经历一个星期惨无人道的论文魔改。git管理的论文目录从20M飙升到50M，一句话就是：你不知道我经历了神马。 七、八、九月疯狂的试用期。试用期的工作强度与实习期就完全是天壤之别，完不成工作进度的我，就经常周末加班了 ┐(—__—)┌。(头发没了，我也变强了——《一拳超人》) 《Elasticsearch 权威指南》 这本电子书帮助我完成了第一个体量比较大的功能，搜索整个公司用户的基本信息。《Hadoop权威指南》、《Hbase权威指南》 为了了解学习使用国内开源Kylin分布式分析引擎（底层使用Hbase进行存储），好对组里实现的魔改版Spark作竞品对比。《css揭秘》 作为有名组里的胶水人物，当然是不只懂得数据相关的知识哇，不过css入门我是不建议看这本书的。 十、十一、十二月转正后的猿人生活。我相信选择成为猿的同志们，生命中总有那么几个契机，引导了你。对我而言，第一道转折是小学时期接触的一款游戏：《重装机兵》(1991由日本游戏公司Crea-Tech出品)，主人公和志同道合的朋友踏上充满危险的冒险之旅，开着男人的浪漫（坦克）拯救世界。然而给我种下这颗种子的并不是主人公，而是最终Boss若亚，身为分布式电脑集群的它，被制造的初衷是为了保护环境。然而病毒感染，程序错乱，导致计算出错，得出了保护地球环境就必须完全消灭人类的结论。好奇心就是这么神奇，为我打开了第一道大门。而这份职业也就是如此神奇能让你保持饥饿，保持愚蠢，哈哈美妙的生活。 《Java并发编程实战》 为开发组内的调度系统，打下基础。《设计模式》 又是一本圣经读物。《JavaScript高级程序设计》 自己业余开始全面的学习前端知识。","link":"/2016/12/29/2016 Year in Review/"},{"title":"Gworm","text":"GwormGworm是一个java版的爬虫框架，以json格式作为返回。 目录 依赖包 参数文件 爬取规则 请求参数 示例程序 初始化 链接生成器 数据操作 异步加载 依赖包 JSON库 : Gson 2.6.2 DOM提取 : jsoup 18.3 WebDriver : selenium-java 2.53.0 1234567891011121314151617181920212223242526272829303132333435363738394041424344#pom.xml&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gyak.gworm&lt;/groupId&gt; &lt;artifactId&gt;gworm&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;1.8.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.seleniumhq.selenium&lt;/groupId&gt; &lt;artifactId&gt;selenium-java&lt;/artifactId&gt; &lt;version&gt;2.53.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 参数文件使用Gworm爬虫框架需要配置两份文件，爬虫规则文件、请求参数文件。 爬取规则123456789101112131415161718192021222324252627//src\\test\\resources\\jd.json[ { \"id\": \"Book\", \"gwormArray\": { \"id\": \"bookList\", \"rule\": \"#plist .gl-item\", \"list\": [ { \"id\": \"bookName\", \"rule\": \".p-name\", \"get\": \"text\" }, { \"id\": \"bookPage\", \"rule\": \".p-name a\", \"get\": \"href\" }, { \"id\": \"bookAuthor\", \"rule\": \".author_type_1 a\", \"get\": \"text\" } ] } }] 请求参数12345Accept=text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Accept-Encoding=gzip, deflate, sdchAccept-Language=zh-CN,zh;q=0.8Connection=keep-aliveUser-Agent=Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.94 Safari/537.36 示例程序通过几段示例程序使用Gworm爬取京东20页的图书信息 初始化GwormBox顾名思义为爬虫箱子，使用单例模式初始化，保存所有的爬虫规则。RequestProperties为整个爬虫框架配置请求参数，addGworm方法添加一套爬取规则。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//src\\test\\java\\com\\gyak\\test\\Test.javaGwormBox gwormBox = GwormBox.getInstance();RequestProperties rp = RequestProperties.getInstance();//REQUEST_FILE 请求参数文件名rp.initProperties(ClassLoader.getSystemResourceAsStream(REQUEST_FILE));//WORM_CONFIG 爬取配置文件名//NAME 对配置设置的别名gwormBox.addGworm(WORM_CONFIG, NAME);``` ### 链接生成器url生成器用来生产爬取目标，方便串行、并行的爬取目标，接口为`UrlGeneration````java//src\\test\\java\\com\\gyak\\test\\Test.javaclass JdUrlGeneration implements UrlGeneration { private int currentPage = 1; private final String page = \"http://list.jd.com/list.html?cat=1713,3258,3297&amp;page=%d&amp;trans=1&amp;JL=6_0_0\"; @Override public int getStart() { return 1; } @Override public int getEnd() { return 20; } @Override public void next() { currentPage ++; } @Override public HasUrl getCurrentbindObj() { return new HasUrl() { private String url = String.format(page, currentPage); @Override public String getUrl() { return url; } }; }} 数据操作GwormCoordinate规则定位器，传入GwormAction中用来指定使用的规则，实现虚函数action用来处理已经爬回的json数据，bindObj对应URL生成器绑定的对象。12345678910111213141516171819202122232425262728293031//src\\test\\java\\com\\gyak\\test\\Test.javaUrlGeneration jd = new JdUrlGeneration();//NAME 爬虫规则的别名//URL_ID 规则中指定链接GwormCoordinate coordinate = new GwormCoordinate(NAME, URL_ID); //concurrency 并发数GwormAction ga = new GwormAction(concurrency, jd, coordinate) { /** * 爬取结果的处理函数 * @param json 爬取的JSON * @param bind 对应url生成器绑定的对象 */ @Override public void action(String json, Object bindObj) { JsonArray jsonArray = json.get(BOOK_LIST).getAsJsonArray(); for (int i=0;i&lt;jsonArray.size();i++) { JsonObject obj = jsonArray.get(i).getAsJsonObject(); String bookName = obj.get(BOOK_NAME).getAsString(); String bookPage = obj.get(BOOK_PAGE).getAsString(); String bookAuthor = obj.get(BOOK_AUTHOR).getAsString(); String bookComment = obj.get(BOOK_COMMENT).getAsString(); System.out.println(\"书名：\" + bookName); System.out.println(\"购买链接：\" + bookPage); System.out.println(\"作者：\" + bookAuthor); System.out.println(\"￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣￣\"); } }};ga.work(); //启动爬取 异步加载对于有些通过异步来加载数据的网页，提取这部分数据必须运行js后方能获得，对于这些网页可以自行实现Htmlable（例如调用Chrome来获取html）接口。 最新添加了ChromeProxy类，实现了调用Chrome来爬取异步加载页面，具体示例详见ChromeProxy1ChromeProxy chromeProxy = new ChromeProxy(\"C:\\\\Users\\\\guiya\\\\Desktop\\\\chromedriver.exe\"); ChromeDriver下载地址 其他WebDriver下载地址 运行测试代码时，请修改为自己WebDriver放置的路径。","link":"/2016/12/10/Gworm/"},{"title":"Merry Christmas！","text":"圣诞节哇，像舶来的美食，又在国内师傅的改良下呈现的糕点，又甜又红，一嘴下去，有些迷醉，我眼里竟是被特赦的囚犯，年底热情洋溢的上街放风。 小时候，我总喜欢，花大笔的银子，来买那些我眼里最古怪，最奇特的会变形的贺卡。送给玩伴和给我抄作业的姑娘。然后等待过年，等待压岁钱填补国库的亏空。 初中高中，家里穷到总算没钱可以给我乱花，圣诞节多半是坐在大会堂里，看表演，看窗外的雪，听各班班草班花们唱类似Jingle Bells的歌曲。 每年都可以是一个好圣诞哇，因为这不是传统节日，你可以开心快乐，你可以悲伤抽风。你可以回忆和朋友一起看的《小鬼当家》。你也可以独自一人以一种独特的方式review一下今年的喜怒哀乐。 今年我长了一对犄角，我有了大目标，也有了小心思。","link":"/2016/12/25/Merry Christmas！/"},{"title":"一团乱麻","text":"饿货很早之前我就确认过一个事实，我不是一个吃货，而是一个彻彻底底的饿货，所谓饿货吃是没有多少讲究的，唯独食量可以和吃货一比，吃货则不同，精挑细选，为了食物可以走南闯北。来到广州的饿货我丝毫没有吃货前来的朝圣感，每日亵渎食物也沾沾自喜，既不会有神灵的惩罚，也不会在乎公司的餐补。直到我点了一份鱼丸。 鱼丸林是我的同事，是一个发型极好的小青年，每每映入我眼帘的永远是他的发型。颜值当然不可能是我们的共同点，但我们一个思想江化，一个江信江疑。无论是代码搭档起来还是使用膜法，乃是绝配。鱼丸事件结束后，他告诉我，广州食物是有“毒”的，所谓毒是需要被中和的，否则一定会从身体的薄弱处爆发出来，不用害怕，这解药遍布于广州大街小巷。 凉茶寻到一家凉茶铺子，并没有林说的那么简单，但为了解毒，这些功夫也不算什么。这救命的凉茶摆在我面前与王老吉有几分相似，难道也是红糖味儿嘛，定要一饮而尽。 红色石头味道让人永生难忘，像用红砖头磨出的分泡制而成，嗓子被这石头堵住，七魂六魄被苦的颠三倒四。然而我的肠胃并没有好，身体更加虚落，精神涣散。 粥在广州万事是离不开吃的，虚落对应的当然是食补。斌是我的前辈，也是我们的司机，无论是开发还是生活遇到麻烦，他就会化身为铜锣湾的浩南，尽显一个扛把子的本色。浩南永远是浩南，变身只差一个契机，而那一天，他准备带我们去长洲收收钱，哦不，尝尝粥。","link":"/2016/12/10/一团乱麻/"},{"title":"云中行走","text":"“生活就应处于生命的边缘，你必须反抗，不循规蹈矩，永不止步，拒绝重复自己，把每一天、每一年、每一个想法视为真正的挑战，这样你就能成为一个走钢丝的人。” “当你呕心沥血的投入工作，却看上去游刃有余，这才是一种艺术。” —— 菲利普·帕特 这同样是我追求的东西，wish me good luck。","link":"/2016/02/01/云中行走/"},{"title":"尘埃落定，一点面试经验","text":"本文只对身处渣校，有一定水平，并且希望通过校招进入第一梯队的IT公司的同学有一丢丢帮助，其他人士可以无视 1.准备工作选择校招蹲点的位置翻遍你感兴趣的所有公司官网，查询校招宣讲路线图，选定一个长期蹲点处（PS：渣校就是这样好公司重来不会来江西的啦┑(￣Д ￣)┍）。 尽量覆盖最多的公司 (你当然不想错过任何一个机会对吧)有熟人介绍地形 （租房点的选择还有各种宣讲教室的位置有熟人指点的话一定会轻松许多） 准备笔试题知识点的复习，这个阶段的复习主要针对于第一轮的笔试，这一轮的笔试考察的知识点会非常的广，本科的操作系统、计算机网络、组成原理、数字逻辑、概率论以及离散数学等等，还有你所报的岗位，一般至少要考察你一门语言的掌握程度。在知识点这么广的情况下我是推荐去一些网站上做一做大公司们往年的面试题，首先复习效率比较高（当然不是叫大家去背题、刷题啊，是要完全理解）。其次熟悉面试题的类型，到时候考试就不会莫名其妙了。 项目准备项目可以是你的毕业设计或者平时开发的一些轮子甚至是编程马拉松之类的比赛作品，累计到三个能拿的出手的作品将极大提高你的竞争力，这一部分就主要是毕业前的积累。 2.面试心得选定蹲点处后，开始每天跑宣讲会，对于不是本地的公司一般第一轮笔试在宣讲会结束后马上开始，第一轮技术面试也要看是不是本地公司，不是的话一般第二天就会立马开始，技术通过会紧接着HR面，而本地公司时间将会被拉长，每次面试三天到一个星期都是有可能的。 第一轮笔试我建议在笔试前一天再看一看这家公司往年的面试题，因为一个公司的题目风格基本是不会有太大变化的，可以说是换汤不换药，温习一遍有益无害。 技术面试我第一次技术面试是献给了美图公司，在一个小咖啡馆里进行，面试官很和蔼很nice，先是叫我做一个自我介绍，然后谈谈自己做过哪些项目，通过你的自我介绍和项目面试官大概知道你熟悉哪些方面开发，为了验证你真实性，最后会问你许多相关的技术问题，如果之前有编造的话是不可能不露出马脚的（PS:能来面你的技术人员都是有能力完爆应届生的）。 如果以你的能力可以顶住最后的提问基本上这个offer拿到了一半（有一两个不会都因该是可以的以我的经验），在接下来的技术二面难度应该是跟第一轮持平的，不过在二面中更倾向于查漏，面试官会更关注第一轮中没有考察或者答错的方面，比如第一轮没有怎么问算法题，这一轮很可能就是算法的轰炸了。 HR面HR面被刷也是很有可能的，不过我要说的是只要你性格不奇葩到令人发指，被刷的原因就都不在HR面，而是前几轮的综合分数偏低，在横向比较下靠后。所以面HR只要正常发挥就好，根本没什么技巧，不过要是问道薪资的问题，我建议是遇到十分想进的公司可以有所让步，或者把皮球退给HR，说公式可以按我的能力开薪资，其他就可以坚持一下，自信甩出一句“我的能力值这个价” 技术总监面这一面主要看公司，有少数公司在HR面后加最后一轮总监，面试过程跟前面的技术面差不多，但是基本上会被碾压的很厉害，这一轮的目的有两个。 这个总监很可能就是你未来的顶头上司，他来看看这个人到底顺不顺眼给你展示总监的技术水平，全面的碾压你，告诉你你选择本公司有很大的学习空间最后祝愿大家都找到满意的工作 o(￣▽￣)ブ","link":"/2015/11/03/尘埃落定，一点面试经验/"},{"title":"台风过后一片狼藉","text":"台风过后，和我这几天的心境一样一片狼藉 第一次亲身经历如此强大的自然力量，人哇如此渺小、微不足道，漫步回家的路上，是一片狼藉，回忆起几天前还是如此安稳僻静的前湾，如今一切都东倒西歪，路面上挤满了泥沙。这就是经历洗礼的样子哇。 和我的心境一般，一通乱麻，在面对我自己一直扬言要追寻的挑战时，我颤栗了，比我想象中要来的快太多，我好像还没有准备好，接受这种洗礼。多想躲在自己的舒适区里，安稳僻静。 写代码总是在徘徊，我不敢确定它的形状，到底要用什么策略。老大的建议是我讨厌的方式，是不是要反驳?还是默默的一意孤行。 保安大哥：“这门禁都吹坏了，不要重新装一套？”工地师傅：”重新搭根线过去，哪有什么问题”便利店阿姨：“阿贵，这棵树明天重新种一下啊，挡在门口怎么做生意” 重新来一下呗！我原来太怕失败了，好像我已经没有时间、没有资格重新再来。职场上跌跌撞撞，麻烦不断又如何呢。好了，我决定了，就这么写了，性能不行又怎样，重新再来呗。 半年后前湾估摸着又开始安稳僻静了吧，我的 Indexing platform 会有人用么，管不了那么多了。","link":"/2017/08/23/2017/台风过后一片狼藉/"},{"title":"前人挖坑后人被埋","text":"本文祭奠我最近两次掉入前人挖的深坑的经历，通过讲述我悲惨的命运故事，来警示新人们在交接项目时如何避免陷阱，如何优雅的躲过被埋的风险。 警惕并不熟练的新技术当你交接的项目里有你并不熟练的新技术时要异常小心，因为最麻烦、最辣手的问题多半是发生在你目前并不了解的区域，首先需要杜绝的就是发生这样的大麻烦。 案例一：最近交接了一个报表系统，前辈也是把代复用、部署简单发挥到了极致。谁愿意为每增加一个报表编写一个 action，然后在编译部署呢？，聪明的老员工选择了使用 Groovy 来解决麻烦，整个报表项目系统的服务 action 只有一个，就是通过参数读取相应的 Groovy 脚本，调用脚本约定的共有执行方法，将结果返回。这样一来添加报表只要对应的实现脚本就好了，系统连重启也不需要，是不是很轻松。 在这之前我对 Groovy 的认识只停留在这是一门基于 JVM 的动态语言，由于完全兼容 Java 的语法，我完全没有把这个项目中的定时炸弹当回事。随着业务量的增加，查询量和结果量都成指数级上升。项目开始不停的内存溢出。直到这时手忙脚乱的开始查询资料，我才发现 Groovy 在为动态场景带来便利的同时，也带来了许多使用上的陷阱。 比如 使用全局的 GroovyClassLoader 导致 Class 无法被卸载，最终永久代被撑满。 groovyLoader.parseClass(groovyScript) 为了保证每次都执行最新的脚本，每次解析也将生成一个新的 Class，这也是一个撑爆永久代的常见原因。 Groovy 脚本在执行了很多次后都会被JVM编译为 native 进行优化，会占据一些 CodeCache 空间，而如果这样的脚本很多的话，可能会导致 CodeCache 被用满，而 CodeCache 一旦被用满，JVM 的 Compiler 就会被禁用，那性能下降的就不是一点点了。 所以在交接新项目的时候最要警惕的就是陌生的技术，可能表面上非常安稳，但完全可能里面却包含了重磅炸弹（小群损失12瓶酸奶安抚受灾群众）。 检验代码仓库和生产环境这是一个经常被忽视的问题，谁会想到生产环境跟源码是不一致的呢，当你像我这样被坑的吐血之后，也一定会对此非常敏感。 案例二：这是一个上个月接手的项目，远古级代码，号称起源在5、6年前，使用 yii 框架的 PHP 项目。由于组里只有勤奋好学的我掌握 PHP (自夸一波)，自然这个坑又落到了我的手里。面对远古项目我不敢怠慢，程序的模块，设计思路都统统进行了分析。在心有成竹之后，得到只要维护这个项目，而不需要改进的命令后更是松了口气。一个月后，运维哥哥找上门来，你这个项目里好多慢 SQL 啊，改进一下哇。额是么，看看问题出处，随手就是一波修改，在我沾沾自喜时，却被运维哥哥怼了一波，你到底改了没改，数据库日志里还是原来的慢 SQL 哇，还有这种事？我也是满脑袋疑惑，等我顺藤摸瓜，找到生产环境时也是被震惊了，apc.php 进入了我的眼帘，难道是被缓存了，心里还小声吐槽，交接文档也没说有这玩意啊，clear cache 这下完美了吧。谁知道整个项目就这样崩溃了，再起不能。我也是满头大汗出个重大事故，哥的效绩要完蛋啊，查了一波异常日志，纳尼数据库连不上，赶紧去运维平台查查，接下来的一切彻底让我的三观崩溃了，对应的数据库在一个月前就已经下线了，那这个项目原来一直依靠着缓存进行运行，我的天，没有人知道真正的数据库地址在哪？源码里的地址早就下线了（交接给我之前项目在一个实习的妹子手里转了一趟，她丝毫没有察觉到 APC 的存在，吐槽她也是无济于事）。就这样一个巨坑呈现在我的面前。还好我比较冷静，通过测试环境抓包分析出了数据库所在地址，没有闹出大问题。 看到这里你有没有开始冒冷汗，当你交接一个项目的时候千万不要把对方当一个正常人来看待，他可能就是个恶魔，还是一个非常粗心的恶魔。检查生产环境和源码是否一致，千万不要像我这样，惊心动魄。","link":"/2017/08/20/2017/前人挖坑后人被埋/"},{"title":"夜深斜月印窗纱","text":"不知不觉，后知后觉，我已经是一个奔三的人了。没法立马横刀，狂言乱笑，妄谈人生了。因为我已经落到了曾经的妄谈之中，被时间推到了草场上，我也不知道自己是骡子是马，硬是被拉出来遛这一遭。 江湖啊跟想象之中没啥两样，的确是林子大，鸟多，虫多，然后是要早起。我就这么的随波逐流，随处风流的来到了这网里。这会儿意气风发的病还没有治好，跟土著们大谈特谈人生几何，颠覆乾坤，如何如何错综复杂，如何如千头万绪，再如何如何的妙趣横生，纵使祖师爷转世也要横眉冷对几句的架势。但土著们毕竟是见多识广，我这种青头是见得多了，也不怎么辩驳，只是打开网页，翻起股票来。 跟财务自由的土著们比武，之后是输的很惨的，这时的江湖才显露出了真江湖。刀光剑影没有几锭银子有杀伤力，武林秘籍没有老大在饭桌上浅谈的炒房秘技来的有市场，谈爱恨，红尘会笑哇。 我知道我只不过是江湖里的路人甲。不收起年轻的姿态，收起幼稚的锋芒来，只会越发的尴尬。但做了咸鱼就不要想着翻身，这叫不务正业。 夜深斜月印窗纱，孤寂的长夜里没有沉默的死亡，那便是要爆发，那便是我花开时百花杀。","link":"/2017/06/16/2017/夜深斜月印窗纱/"},{"title":"思考需要交融","text":"一个小时前，我在微博上刷到一篇【四大名著之一的《水浒传》好在哪里？】的回答，回答很长，内容非常多，重点是作者对小说的解读对我有一种醍醐灌顶的感觉。作者提出的很多点我都没有思考过，但读过之后又那么的感觉理所当然，非常认同。 我似乎明白了一件事情，那就是思考需要热烈、需要交融。在读技术书的时候往往我会去想，这个知识对我意味着什么，这个对工程师们意味着什么，这是一个息息相关的东西，我和它交融在一起。而当我刷起知乎、微博的时候不断的有碎片化信息进入大脑，从庞大的信息群里苛求那么一两个能让我兴奋的片段，大脑开始越来越麻木，机械化，放弃思考，放弃自己应有的代入感。当你放弃与你接触的信息发生化学反应的时候，信息悄无声息与冰冷，只是单纯的流逝时间，之后真的遇到需要对这些信息进行call back的情况时，往往是不灵的。 怪不得我无法学好我不感兴趣的东西，这些东西天生的让我感觉与我无关，我放弃跟它的所有交流。理科的思维总是不停地对那个已经深入脑髓的“逻辑框架”内的修修补补感到兴奋，每一次改动或者增加都对我的世界观进行了patch，与工作生活太相关了（也许因此占用了大量的CPU时间）。而政经类的信息，虽然每天总要遇到大量的输入，但是貌似从输入那一刻开始就被标记为了与我无关的标签，导致没法回调。 既然如此，学习不感兴趣的知识的方法就变的很简单了，就是要对它产生兴趣（果然是废话挖），可能你会说这谈何容易哇，不我觉得还是有捷径的，就是把他与你目前最感兴趣的东西进行关联。我目前最感兴趣的就是$。","link":"/2017/06/19/2017/思考需要交融/"},{"title":"暴雪哈希去重器","text":"去重器在做数据处理的时候，去重是非常常用的一种操作，通过Java语言去做去重，一般直接想到使用HashSet，Java的HashSet内部其实还是一个HashMap，如果看过HashSet的源码，你会发现add一个元素e，其实内部是map.put(e, PRESENT)，PRESENT是一个公用的Object，通过返回值来判断e是否存在在map里，所以HashSet内部存储你可以等价于一个HashMap，只不过所有的Value是一个公用的Object对象。 那我接下来谈谈HashMap的实现，HashMap内部有一个哈希桶数组Entry&lt;K,V&gt;[] table，每来一对K/V，会对K进行哈希，哈希值进行取模对应到table中的一个位置，如果桶里已经存在V，那么就发生了Hash冲突，桶的大小是有限，Hash能把数据打散，但冲突在所难免，这时候每个桶里对应的就不是单纯的V，而是一个链表（Java8中又进行的优化，当链表长度到阈值，会变形成红黑树，取值时的搜索速度） 当你碰到内存的问题你已经了解到HashSet、HashMap的内部存储，你会发现你不得不记录所有的K/V，因为你这些容器还要提供给你get的操作，但是当你只需要一个去重功能的时候，你的内存非常珍惜的时候，使用HashSet去去重是不明智的。 如果有一种神奇的Hash算法，对于每一个Key，都能计算出一个绝不冲突的哈希值，那么问题就简单了，我只要记录这个哈希值就好了，需要的存储就小了很多。 有没有银弹呢？我最近发现了一种哈希算法：暴雪哈希 https://www.biaodianfu.com/blizzard-hash.html暴雪公司的魔兽、星际等游戏都一样一个非常大的MPQ文件，该文件存储了游戏中的大部分数据，想要把这些文字找出来，简单的办法是从数组头开始，一个个字符串读过去，比较每一个，直到找到对应的内容。Blizzard的天才和牛人们当然不会这样做，他们用了更聪明的方法: 用某种算法，把一个字符串压缩成一个整数，即hash。然后，根据这个整数值，直接得到此字符串在整个文件中的位置，从而直接读取之。 这个Hash算法非常高效具体实现在http://sfsrealm.hopto.org/inside_mopaq/chapter2.htm，内部计算了三个哈希值，一个当作偏移量，另两个效验。三者同时冲突的概率为1:18889465931478580854784，百亿亿分之一，这个冲突概率在很多数据处理中完全是可以接受的（收集的数据中出现噪声的概率要比这高的多）。 下面是我通过暴雪Hash实现的去重器，每个key只记录哈希值，无法还原原值，但是存储要比HashSet小的多。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150public class BlizzardHashSet { private final int DEFAULT_INITIAL_CAPACITY = 200; private int size = 0; private Entry[] table; private int cryptTable[]; public BlizzardHashSet() { initTable(); } private void initTable() { int capacity = DEFAULT_INITIAL_CAPACITY; this.table = new Entry[capacity]; initCryptTable(); } /** * return number of keys in hash table * * @return number of keys */ public int size() { return size; } private void checkSize() { if (2 * size &gt; table.length) { int newSize = table.length &lt;&lt; 1; Entry[] newTable = new Entry[newSize]; for (Entry e : table) { while (e != null) { int pos = Math.abs(e.getHash() % newSize); if (newTable[pos] == null) { newTable[pos] = e; } else { Entry current = newTable[pos]; Entry next = current.next; do { if (next == null) { current.next = e; break; } else { current = next; next = current.next; } } while (true); } Entry temp = e.next; if (temp != null) { e.next = null; } e = temp; } } table = newTable; } } public boolean add(String str) { Entry e = new Entry(str); int hash = e.getHash(); int pos = Math.abs(hash % table.length); if (table[pos] == null) { table[pos] = e; size ++; checkSize(); return true; } else { Entry oldEntry = table[pos]; while (true) { if (checkHash(oldEntry, e)) { return false; } if (oldEntry.next == null) { oldEntry.next = e; size ++; checkSize(); return true; } oldEntry = oldEntry.next; } } } private boolean checkHash(Entry a, Entry b) { return !(a.hash != b.hash || a.hashA != b.hashA || a.hashB != b.hashB); } /** * Tests if this hashtable maps no keys to values. * * @return hash table is empty or not */ public boolean isEmpty() { return size == 0; } private void initCryptTable() { this.cryptTable = new int[0x500]; int seed = 0x00100001; int index1 = 0, index2 = 0, i; for (index1 = 0; index1 &lt; 0x100; index1++) { for (index2 = index1, i = 0; i &lt; 5; i++, index2 += 0x100) { int temp1, temp2; seed = (seed * 125 + 3) % 0x2AAAAB; temp1 = (seed &amp; 0xFFFF) &lt;&lt; 0x10; seed = (seed * 125 + 3) % 0x2AAAAB; temp2 = (seed &amp; 0xFFFF); cryptTable[index2] = (temp1 | temp2); } } } /** * Uses MPQ hash algorithm to generate a long type variable as hashed value. * This version only accepts String. * * @param key * receive a string as key in hash map. * @return returns a long type argument as hashed value */ private int hash(String key, int dwHashType) { int seed1 = 0x7FED7FED, seed2 = 0xEEEEEEEE; for (int i = 0; i &lt; key.length(); i++) { int ch = key.charAt(i); seed1 = cryptTable[(dwHashType &lt;&lt; 8) + ch] ^ (seed1 + seed2); seed2 = ch + seed1 + seed2 + (seed2 &lt;&lt; 5) + 3; } return seed1; } class Entry { int hash,hashA,hashB; Entry next; /** * Creates new entry. */ Entry(String string) { hash = hash(string, 1); hashA = hash(string, 2); hashB = hash(string, 3); } public int getHash() { return hash; } public int getHashA() { return hashA; } public int getHashB() { return hashB; } public void setNext(Entry e) { next = e; } }}","link":"/2017/04/09/2017/暴雪哈希去重器/"},{"title":"构建自己的日志索引系统（序言）","text":"相隔4个月没有写技术博客了，还真是有点懒惰啊，蛤蛤，其实我还是有很多借口的哇。这半年都从零独立开发公司的日志索引系统。踩了很多坑，也有一些小的技术突破，所以准备写一篇叫《构建自己的日志索引系统》的系列文章，记录与讨论一下我采用的技术方案。 这一篇作为序言，那先讲讲这个系统的前世今生吧，为什么要做日志索引，对于一个产品来说，日志是一个监控诊断应用健康状况，帮助反馈信息的重要途径，一个数据团队是不可以缺少日志检索这个体系的。目前市面上最主流的方案是ELK(ElasticSearch + Logstash + Kibana)，ElasticSearch 进行数据索引，Logstash 用于数据收集，Kibana 用于数据分析。那我们为什么要重新造轮子呢，为什么不拿来主义呢。因为测试 ElasticSearch 的索引性能结果并不满意，单机测试经过了一系列的调优极限差不多3W条日志/每秒，并且CPU占用这个情况下也非常高，如果按照这个吞吐量进行部署将需要一个比较大的集群，性价比非常不划算。我们希望这个日志系统能与我们的 Hadoop 集群相融合，背靠大山，吃喝不愁，并且他们的计算高峰点正好是互相错开的（日志系统高峰在前夜，离线集群跑数在凌晨），处在同一个集群不会有太大的影响，由此来共用机器降低成本。 在降低成本的前提下还需要有较高的索引性能，所以我的最终方案是，重写 ELasticSearch，当然重写的过程中我做了许多取舍，首先去除了 ES (后面都简写)的写事务，写事务是一个非常影响索引性能的点，ES 在写索引的同时还要进行事务日志的写入，保证日志的写入后再进行正真的 commit。那我如何保证数据安全的落地呢？这一块数据的维持就交给了 Kafka 来完成。第二去除 ES 中相对日志索引来说多余的功能，例如使用了更精简的元数据设计，不在出现 ES 中 _source、_all 字段。另外在查询方面并没有保留 ES search 的 DSL 风格，还是改为深入人心的 SQL 语法，一方面日志检索系统，并没用到 Lucene 的打分机制，都是进行是否匹配的布尔查询，对于模糊查询等搜索引擎系统需要的功能，我们只需要其精确匹配的部分，使用 SQL 表达力上也非常足够。最后存储方面并不像 ES 集群写入本地磁盘，而是改为 HDFS 文件系统，保证数据备份也增大了集群的吞吐量，磁盘 IO 转换为网络 IO，在万兆网卡下，索引的瓶颈也从 IO 转变到单机的 CPU 使用，这一步分 Directory 的抽象就直接使用了 Solr 的代码。 日志索引平台数据拉取架构 设想了一下接下来大概会连载如下六个章节 元数据的设计日志索引，不同于业务的数据，在数据内容方面字段的更改比较频繁，而且单条日志来说字段数并不统一。在不数据量上要考虑到伸缩扩展性。基于这两方面阐述一下具有时间线的动态元数据如何设计与实现。 数据恢复机制分布式服务，难免会有单节点故障，网络或者磁盘损坏等原因都有可能发生，如何在发生故障后恢复数据，从正确的位置上重新拉取 Kafka？ 近实时索引，内存与hdfs的两段式写入Lucene 的索引写入分为好几个阶段，其中对磁盘的 flush 与 commit 是非常耗时的过程，所以直接对 hdfs 文件系统写入索引，很难做到近实时，需要等待文档堆积到一定数量之后引发 flush / commit，之后才能查询到这之间的数据。为了做到近实时，就要利用到 RamDirectory，在内存中先进行索引，再批量刷新进入磁盘。 弹性伸缩为了保证集群的高可用，必须能应对特殊事件造成突如其来的访问高峰，能随着数据量的增多自动扩容，随着高峰退去，自动削减机器。可谓“他强由他强，清风拂山岗；他横由他横，明月照大江。” SQL解析器与执行计划为了提供一个业务方能非常快接受的服务，使用大家都非常熟悉的 SQL 语句就是很好的选择。这一篇计划讲解一下如何开发 SQL 解析器来映射到 Lucene Query，查询逻辑如何分发到集群，子节点如何高效的执行逻辑计划。 RPC与序列化相对于单机服务来说，分布式服务增加了机器之间的通讯，这就必须使用到 RPC 服务，服务数据又涉及到对象的序列化。这一块的实现也会大大影响到查询的性能。 先哇这么多坑，争取过年前把坑填完。（逃。。。","link":"/2017/12/23/2017/构建自己的日志索引系统（序言）/"},{"title":"用code打造自己的过渡动画","text":"起源自己的过渡动画，为什么要这个东西呢？不是说好的Simple is beautiful么。的确我也是非常欣赏简洁的东西，但是对于挂载在Github上的本博客，在国内访问总是不那么流畅，而Next主题的文字下落动画恰恰使博客访问体验变的非常糟糕（感觉访问变得更慢了）。所以本喵决定打造自己的过渡动画。 借鉴在两三个月前记得访问过一个个人博客，过渡动画非常惊艳，但当时并没有时间详细琢磨实现细节。如今正好借鉴一番，经过一番Google，还是本喵找到了（但是作者的博客好像没有维护了，无法访问），好在也是依托Github pages 挂载的博客，直接把源码下载下来进行研究。 地址： https://github.com/ceoimon/ceoimon.github.io 实现细节 动画并非纯css实现而是使用了SVG + CSS 可缩放矢量图形（英语：Scalable Vector Graphics，SVG）是一种基于可扩展标记语言（XML），用于描述二维矢量图形的图形格式。 SVG由W3C制定，是一个开放标准。既然是矢量图，它的缩放就不会失真，因为表示方法是基于数学的表示，而不是像素点。动画中的笔画即是基于SVG中path标签中的数据先后顺序。 123456789101112131415161718192021222324252627282930313233#mySVG path { stroke: #000; stroke-width: 2px; stroke-dasharray: 11434; stroke-dashoffset: 11456; -webkit-animation: dash 5s linear infinite; animation: dash 5s linear infinite; -webkit-animation-fill-mode: both; animation-fill-mode: both;/* -webkit-transition:fill 0.5s 1s linear; transition:fill 0.5s 1s linear;*/}@-webkit-keyframes dash { 10% { stroke-dashoffset: 11456; } 70% { stroke-dashoffset: 0; fill: transparent; } 80% { stroke-dashoffset: 0; fill: #000; } 95% { stroke-dashoffset: 11456; fill: #000; } 100% { stroke-dashoffset: 11456; fill: transparent; }} 不过当我理解了这svg两个属性(stroke-dasharray，stroke-dashoffset)后，还是非常震惊于这个动画的实现原理。 stroke-dasharray属性可控制用来描边的点划线的图案范式。 MDN给出的示例： 12345678910111213141516171819&lt;?xml version=\"1.0\"?&gt;&lt;svg width=\"200\" height=\"200\" viewPort=\"0 0 200 300\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\"&gt; &lt;line stroke-dasharray=\"5, 5\" x1=\"10\" y1=\"10\" x2=\"190\" y2=\"10\" /&gt; &lt;line stroke-dasharray=\"5, 10\" x1=\"10\" y1=\"30\" x2=\"190\" y2=\"30\" /&gt; &lt;line stroke-dasharray=\"10, 5\" x1=\"10\" y1=\"50\" x2=\"190\" y2=\"50\" /&gt; &lt;line stroke-dasharray=\"5, 1\" x1=\"10\" y1=\"70\" x2=\"190\" y2=\"70\" /&gt; &lt;line stroke-dasharray=\"1, 5\" x1=\"10\" y1=\"90\" x2=\"190\" y2=\"90\" /&gt; &lt;line stroke-dasharray=\"0.9\" x1=\"10\" y1=\"110\" x2=\"190\" y2=\"110\" /&gt; &lt;line stroke-dasharray=\"15, 10, 5\" x1=\"10\" y1=\"130\" x2=\"190\" y2=\"130\" /&gt; &lt;line stroke-dasharray=\"15, 10, 5, 10\" x1=\"10\" y1=\"150\" x2=\"190\" y2=\"150\" /&gt; &lt;line stroke-dasharray=\"15, 10, 5, 10, 15\" x1=\"10\" y1=\"170\" x2=\"190\" y2=\"170\" /&gt; &lt;line stroke-dasharray=\"5, 5, 1, 5\" x1=\"10\" y1=\"190\" x2=\"190\" y2=\"190\" /&gt;&lt;style&gt;&lt;![CDATA[line{ stroke: black; stroke-width: 2;}]]&gt;&lt;/style&gt;&lt;/svg&gt; line{stroke:black;stroke-width: 2;} stroke-dashoffset属性指定了dash模式到路径开始的距离 通过CSS修改这两个属性，动画的边界就相应的动起来了。 我的实现我的素材： 使用 AI 得到 SVG，最终效果： GeekCat体验","link":"/2017/01/29/2017/用code打造自己的过渡动画/"},{"title":"构造自定义领域语言（一）","text":"前言最近的工作一直在开发公司内部的数据查询平台，随着查询引擎的增加，后台的复杂度徒增。使用单纯的SQL拼接来生成不同的SQL方言已经很难维护。个人主张设计一套自定义的领域语言，用作界面查询的表达，后台统一翻译成IR(Intermediate language)，再通过不同的翻译器生成不同平台的SQL语言。 例如神策系统中这段中文在后台需要转化成不同查询引擎的SQL。但是大型系统就像马拉的战车，系统越大，马越多就越难拐弯。所以个人想先使用业余时间做一番实践，观察结果，再考虑生成环境的转型。 工具工欲善其事必先利其器，综合考虑使用ANTLR工具来进行实践（Hive SQL的解析就是使用的ANTLR工具，ANTLR可以通过简洁的语法生成解析树、AST）。 ANTLR ANTLR官网GQL 准备通过这个项目来同步学习实践的代码。编程语言实现模式、The Definitive ANTLR 4 Reference 学习书籍 解析起步ANTLR使用递归下降的方式解析语言，《编程语言实现模式》中前四种模式： 模式一：从文法到递归下降识别器（无法解决左递归文法，每层递归都无法消解任何词语），文法中每一个规则对应一个Parser类。 1234567public class G extends Parser { /** * 词法单元类型的定义 * 合适的构造函数 * 规则对应的方法 */} 模式二：LL(1)递归下降的词法解析器，每次向前看一个字符，一次token一个词法单元。模式三：LL(1)递归下降的语法解析器，每次向前看一个词法单元，通过语法规则前缀判断如何应用规则（对于多个规则有公共左子式，向前读一个词法单元就无能为力了）。模式四：LL(K)递归下降的语法解析器，相比模式三，向前读的词法单元变成了K个。只要K大于公共左子式的长度，就可以解析下去。 解析实践 1234567891011121314151617grammar GQL;gql_behavior : &apos;按&apos; cycle statistics;cycle : &apos;每天&apos; | &apos;每月&apos; | &apos;每小时&apos; | &apos;每分钟&apos; ;statistics : &apos;统计&apos; index groups;index : table &apos;的&apos; columns;table : (~(&apos;的&apos;))+;columns : column (&apos;和&apos; columns)*;column : ~(&apos;的&apos;|&apos;和&apos;|&apos;按&apos;)+;groups : group+;group : &apos;按&apos; groupItem &apos;查看&apos;;groupItem : ANY_CHAR+;ANY_CHAR : .;WS : [ \\t\\r\\n]+ -&gt; skip ; 匹配“按每天统计用户表的触发用户数和触发用户总次数按渠道查看按时间查看” 下星期继续更新实践","link":"/2017/01/02/2017/构造自定义领域语言（一）/"},{"title":"Android逆向实践","text":"本文记录一下最近逆向一款 Andoird 应用，获取相关数据的的实践思路与过程。希望对有类似需求的同学有所帮助。 爬取 Web 站点相比爬取 Android 应用要简单一些，原因是通过浏览器调试，你可以比较方便的查看资源的请求信息，request 与 response 都比较清晰，而对于一个 Android 应用就比较难以调试了，首先代码进行了混淆，即便反编译，还是很难理解。抓包是一个很好的思路，但是对于加密的数据包，需要应用内部相应的解码函数你才能获取到明文，不像 web 应用 JS 的解码函数也一览无遗。对于这种情况以下是我的思路与实践方式。 虽然代码混淆了，有总比没有好，先用解压 APK 文件得到如图的文件夹。 DEX 文件其实就是 Android dalvik 虚拟机的执行文件，类比普通 Java 程序编译出的字节码。可以看出这里有多个 DEX 文件说明这个应用的方法比较多，超过了 65535 进行了拆分。接下来使用 dex2jar 工具将 DEX 文件转化为 Jar 包。d2j-dex2jar.sh class*.dex 这将生成四个对应的 Jar 包，我为了阅读方便将四个 Jar 包进行解压目录合并。使用 IDEA 或者其他工具打开就可以看到反编译的代码了，一般应用都会进行混淆，这里包名类名大多都替换成了 a b c …，基本是没法阅读的。完成了第一步代码的反编译，接下来要找到获取你想要的数据的入口，怎么办呢？adb shell dumpsys activity activities 来帮忙。这条命令将会打印出整个系统里的任务栈，你先讲模拟器，或者真机跳跃到你需要爬取数据的相应页面，通过命令打印出任务栈，在之前解压的目录名其实就是这个应用的前缀包名，对任务栈进行搜索，就可以确定当前页面 Activity 的名称了。这里你要知道带界面的应用的代码结构离不开 MVC 的设计。你已经确定了 V 层的代码路径，相应的 M 层的代码也不会太远，进行一些分析可以确定一些可疑的数据类（关注 Activity 混淆后的代码的 import 也有帮助）。这时候就需要想办法 hook 函数输出点什么来确定自己的猜测了。轮到 Xposed 登场，这是一个强大的 Android hook 框架，它的原理是替换 /system/bin/app_process 系统文件从而控制 zygote 进程，使得系统启动是会加载 XposedBridge.jar 这个 jar 包。劫持 Dalvik 虚拟机，它将替换原本的类加载器，在加载类时添加 hook。http://repo.xposed.info/module/de.robv.android.xposed.installer 官网进行下载，注意这里需要区分 Android 系统的版本，5.0之后的 Android 系统修改了加载 DEX 的方式。我使用了网易的MUMU模拟器，是4.4版本的系统，DEX 是分批加载的。首先在模拟器里按照相应的 Xposed installer。 点击框架以激活框架，实际就是替换之前说的系统文件。那我们就要开始开发相应的 hook 模块了。https://github.com/rovo89/XposedInstaller/releases 下载相应的模块开发依赖 jar 包。 这里需要注意的是在 Android Studio 中选择使用 complie_only ，如果把这个依赖包一起打包进入自己的 hook apk 将会与 Xposed 自带的 jar 发生冲突。12345678910111213141516171819202122232425262728293031323334353637383940public class Main implements IXposedHookLoadPackage { @Override public void handleLoadPackage(XC_LoadPackage.LoadPackageParam loadPackageParam) throws Throwable { if (!loadPackageParam.packageName.equals(\"package name\")) { //替换你你需要 hook 的应用包名 return; } XposedBridge.log(\"Loaded app:\" + loadPackageParam.packageName); final String className = \"model class name\"; //替换你你需要 hook 的数据模型类名 XposedHelpers.findAndHookMethod(Application.class, \"attach\", Context.class, new XC_MethodHook() { @Override protected void afterHookedMethod(MethodHookParam param) throws Throwable { ClassLoader cl = ((Context)param.args[0]).getClassLoader(); final Class&lt;?&gt; hookClass; try { hookClass = cl.loadClass(className); } catch (Exception e) { XposedBridge.log(\"no find \" + className); return; } XposedHelpers.findAndHookMethod(hookClass, \"setContent\", String.class, new XC_MethodHook(){ @Override protected void afterHookedMethod(MethodHookParam param) throws Throwable { XposedBridge.log(\"find \" + className); XposedBridge.log(\"param = \" + param.args[0].toString()); } }); } }); }} 由于我是在4.4版本下进行 hook 这里是分批进行 DEX 文件加载的，如果直接进行 XposedHelpers.findAndHookMethod 可能会发生 class not found 的情况，所以这里先进行了 Application.class attach 方法的 hook，在确定了应用上下文已经加载后在进行对应目标的 hook，Xposed 提供了许多 hook 方法这里示例 hook 两个函数，实际上还可以 hook 构造函数等等，详细可以阅读对应的 Api。 接下来启动需要 hook 的应用，就可以在 Xposed 日志页面查看输出，通过输出就可以验证之前的猜测，成功之后就可以依照此法摸清这个调用链，将相应的代码脱离出来，进行爬取。有时间我在更新一部分如何 hook native 的时间。","link":"/2018/01/28/2018/Android逆向实践/"},{"title":"番禺山浣熊厂物语：第一回，小群抢话题吹牛皮，老司机度陈仓教做人","text":"浣熊厂的19点依然是满满的工作氛围，今个也不例外，厂主带领大家围观新系统，左一言又一语，好不快活。小群我自然是要凑这个热闹的。 小群：“这个特性我本来是要装mermaid插件的哇，什么甘特图、流程图、ER图的是小case，但是这个插件的依赖包里需要C++11特性哇，所以哇，现在不行哇…（眉飞色舞，故弄玄虚状）。” 老司机：“哎呀小群不得了哇，看NodeJS了啊，听说这个开发WEB效果很好哇，老人家不懂啊，为什么呢？” 小群：“（这波牛皮我等了7天零15个小时）因为异步、AIO啊，线程在进行IO的时候可以做别的事情啊，所以快啊（内心戏：老司机你也有今天）。” 老司机：“AIO，我怀疑你看了假书啊，小群没事多看看正版资料，window和Linux上都没有实现真正的异步IO。” 小群：“这怎么可能，NodeJS的卖点就是异步呀（难道这回又被光速打脸！！！）” 老司机：“操作系统内核对于I/O只有两种：阻塞与非阻塞。” 小群：“那异步与同步呢？” 老司机：“别插嘴哇，听我慢慢说，阻塞IO造成CPU等待IO，浪费时间，非阻塞IO到用后立刻返回，CPU时间片可以处理其他事情，但是什么时候获取真正的返回呢？” 小群：“要轮询吧，按我的理解换成异步就可以直接获得。” 老司机：“那你先听听这几样非阻塞技术吧，read:最原始的轮询，反复调用来检查IO状态。select:通过对文件描述符上的状态进行判断，有个限制就是它采用1024长度的数组来记录文件描述符（所以一次做多轮询1024个IO是不是已经快了很多），poll:采用链表代替了数组，没了长度限制，但是在文件描述符多的情况下还是没啥性能。epoll:该方案是Linux下最有效率的IO通知机制了，进入轮询但是没有检测到IO事件，将休眠，直到事件发生把它唤醒。” 小群：“epoll已经是最有效率的啦？说好的异步呢？” 老司机：“Linux上说来是有一种AIO但是仅支持内核IO中的O_DIRECT方式读取，无法利用缓存，所以很鸡助哇。” 小群：“那NodeJS是怎么个异步法呢？” 老司机：“线程池模拟异步哇，v0.93这个版本之前都是利用libeio这个库，之后的版本自行实现了线程池模拟异步IO库，window下是利用了IOCP来实现。” 小群：“。。。原来你都懂。。。” 老司机：“发车发车了，下班下班哇。” 留下老司机给的学习资料，深藏功与名。 http://stackoverflow.com/questions/87892/what-is-the-status-of-posix-asynchronous-i-o-aio http://blog.libtorrent.org/2012/10/asynchronous-disk-io/ https://cnodejs.org/topic/4f16442ccae1f4aa270010a7 http://stackoverflow.com/questions/8768083/difference-between-posix-aio-and-libaio-on-linux http://www.wzxue.com/linux-kernel-aio%E8%BF%99%E4%B8%AA%E5%A5%87%E8%91%A9/","link":"/2017/02/26/2017/番禺山浣熊厂物语：第一回，小群抢话题吹牛皮，老司机度陈仓教做人/"},{"title":"如何修改Lucene索引，加速对数据的Distinct、Group By操作","text":"番禺山浣熊厂物语：第二回——按段标签太可恶，魔改索引来加速出场人物：老司机——本喵导师 前情提要老司机打造的魔改版Spark终于开放给内部产品使用了，速度那是刚刚的，可是老司机毕竟是老司机，是见过大世面的人。 老司机：“呐，小群，我昨晚夜观天象，紫微星微发红光～～”。 小群：“说人话”。 老司机：“（严肃脸）有一个重要的任务要交给你哇，你要听清楚了，我们这个魔改版的Spark底层使用Lucene做存储，每天每个产品的数据导入后都会存储在多台机器上（多个region），每个region按照日期产品分开目录存储索引，目前每个目录的索引只有一个Segment，这是为什么了呢？原因是我们的数据要进行各种SQL统计操作，再呈现到运营人员眼里。具体细节我在多啰嗦几句，where条件对存储在索引里的Term做筛选，使用反向索引就可以找到所有的DocId(一行记录一个Doc，一列对应一个Field)，找到了所有DocId集合再进行Distinct、Group By却有点麻烦。如果对每个DocId都去寻找真实的Field值再去进行去重、分组操作显然是非常没有效率的，因为这些上报数据的Field重复率很高，这里就用到了Lucene的新特性SortedDocValuesField，这个域加入Doc后可以产生排序标签ord，这是一个正向索引，Doc—&gt;Term，拿到这个文档的DocId后你就可以拿到这篇文档每个域真实值的ord，真实值相同，那么ord也必然相同，获取ord要比获取真实值有效率的多，那么就可以使用ord来完成Distinct、和Group By操作了。” 小群：“你还没有说清楚为什么目前只有一个segment呢？” 老司机：“那就要谈谈这个SortedDocValuesField了，这个特性产生的ord并不是全局的，而是分segment的，比如你存储了0,1,2,3,4在segment1中，它们的标签分别是0,1,2,3,4，存储1,2,3,4在segment2中它们的标签却是0,1,2,3，相同的值4在segment1与segment2中的标签却分别是4与3，原来ord的值只是在自己的segment里排序而已。所以目前只使用一个segment，并没有达到最好的效果哇” 小群：“那交给我的任务就是，修改Lucene的源码，完成这个全局ord咯” 老司机：“聪明” 小群：“（鄙视脸）” Lucene（5.2.2）源码解析首先测试用例验证老司机的话1234567891011121314151617181920212223242526272829303132333435363738@Testpublic void test01() throws Exception { Analyzer analyzer = new StandardAnalyzer(); Directory directory = new NIOFSDirectory(Paths.get(\"d://lucene_index\")); IndexWriterConfig config = new IndexWriterConfig(analyzer); config.setMaxBufferedDocs(10); config.setMergePolicy(NoMergePolicy.INSTANCE); config.setUseCompoundFile(false); IndexWriter writer = new IndexWriter(directory, config); Random random = new Random(); for (int i = 0; i &lt; 1000; i++) { int v = random.nextInt(5) + 16; Document doc = new Document(); doc.add(new StringField(\"f1\", \"v\" + String.valueOf(v), Field.Store.YES)); doc.add(new SortedDocValuesField(\"f1\", new BytesRef(\"v\" + String.valueOf(v)))); writer.addDocument(doc); } writer.close();}@Testpublic void test02() throws Exception { Directory directory = new NIOFSDirectory(Paths.get(\"d://lucene_index\")); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query q1 = new TermQuery(new Term(\"f1\", \"v\" + \"20\")); TopDocs td1 = searcher.search(q1, 10); for (ScoreDoc docs : td1.scoreDocs) { int readerIndex = ReaderUtil.subIndex(docs.doc, searcher.getIndexReader().leaves()); LeafReaderContext leafReader = searcher.getIndexReader().leaves().get(readerIndex); SortedDocValues docValues = DocValues.getSorted(leafReader.reader(), \"f1\"); int ord = docValues.getOrd(docs.doc - leafReader.docBase); System.out.println(\"valueCount : \" + docValues.getValueCount()); System.out.println(\"ord : \" + ord); String value = docValues.lookupOrd(ord).utf8ToString(); System.out.println(\"value : \" + value); System.out.println(\"-------------\"); }} 12345678910111213141516171819202122232425262728293031323334353637383940valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 5ord : 4value : v20-------------valueCount : 4ord : 3value : v20-------------valueCount : 5ord : 4value : v20------------- 随机生成v16-v20，查询10个segment，ord标签既有4也有3，验证正确。 解析源码在默认的索引链DefaultIndexingChain中有一个索引DocValue的方法，它为不同的类型提供不同的写者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final class DefaultIndexingChain extends DocConsumer { private void indexDocValue(PerField fp, DocValuesType dvType, IndexableField field) throws IOException { if (fp.fieldInfo.getDocValuesType() == DocValuesType.NONE) { // This is the first time we are seeing this field indexed with doc values, so we // now record the DV type so that any future attempt to (illegally) change // the DV type of this field, will throw an IllegalArgExc: fieldInfos.globalFieldNumbers.setDocValuesType(fp.fieldInfo.number, fp.fieldInfo.name, dvType); } fp.fieldInfo.setDocValuesType(dvType); int docID = docState.docID; switch(dvType) { case NUMERIC: if (fp.docValuesWriter == null) { fp.docValuesWriter = new NumericDocValuesWriter(fp.fieldInfo, bytesUsed); } ((NumericDocValuesWriter) fp.docValuesWriter).addValue(docID, field.numericValue().longValue()); break; case BINARY: if (fp.docValuesWriter == null) { fp.docValuesWriter = new BinaryDocValuesWriter(fp.fieldInfo, bytesUsed); } ((BinaryDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue()); break; case SORTED: if (fp.docValuesWriter == null) { fp.docValuesWriter = new SortedDocValuesWriter(fp.fieldInfo, bytesUsed); } ((SortedDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue()); break; case SORTED_NUMERIC: if (fp.docValuesWriter == null) { fp.docValuesWriter = new SortedNumericDocValuesWriter(fp.fieldInfo, bytesUsed); } ((SortedNumericDocValuesWriter) fp.docValuesWriter).addValue(docID, field.numericValue().longValue()); break; case SORTED_SET: if (fp.docValuesWriter == null) { fp.docValuesWriter = new SortedSetDocValuesWriter(fp.fieldInfo, bytesUsed); } ((SortedSetDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue()); break; default: throw new AssertionError(\"unrecognized DocValues.Type: \" + dvType); } }} 我没主要来看看SortedDocValuesWriter，其他写者逻辑类似 123456789101112131415161718192021class SortedDocValuesWriter extends DocValuesWriter { final BytesRefHash hash; //BytesRef的哈希桶，被写入的value都会转化成BytesRef private PackedLongValues.Builder pending; //用以存储Long值，底层涉及到压缩算法，真正用途后面再讲 private final Counter iwBytesUsed; private long bytesUsed; // this currently only tracks differences in 'pending' private final FieldInfo fieldInfo; //域信息 private static final int EMPTY_ORD = -1; public SortedDocValuesWriter(FieldInfo fieldInfo, Counter iwBytesUsed) { this.fieldInfo = fieldInfo; this.iwBytesUsed = iwBytesUsed; hash = new BytesRefHash( new ByteBlockPool( new ByteBlockPool.DirectTrackingAllocator(iwBytesUsed)), BytesRefHash.DEFAULT_CAPACITY, new DirectBytesStartArray(BytesRefHash.DEFAULT_CAPACITY, iwBytesUsed)); pending = PackedLongValues.deltaPackedBuilder(PackedInts.COMPACT); bytesUsed = pending.ramBytesUsed(); iwBytesUsed.addAndGet(bytesUsed); } .....} 在DefaultIndexingChain中通过调用((SortedSetDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue())向写者加入文档id以及域对应的值，在addValue方法中主要进行一些值的check，入后调用了addOneValue，主要逻辑便在里面。 123456789101112131415private void addOneValue(BytesRef value) { int termID = hash.add(value); //把值加入哈希桶便会返回这个值的termID if (termID &lt; 0) { //如果哈希桶中已经有这个值便返回-（counter+1），这里正好把它恢复，并不增加iwBytesUsed termID = -termID-1; } else { //新值增加iwBytesUsed的使用 // reserve additional space for each unique value: // 1. when indexing, when hash is 50% full, rehash() suddenly needs 2*size ints. // TODO: can this same OOM happen in THPF? // 2. when flushing, we need 1 int per value (slot in the ordMap). iwBytesUsed.addAndGet(2 * RamUsageEstimator.NUM_BYTES_INT); } pending.add(termID); //添加termID到pending中 updateBytesUsed(); } 随着field的加入，达到这个segment的maxDoc时，进行刷新，将内存中的信息写入到磁盘里，对应与flush方法。 123456789101112131415161718192021222324252627282930@Overridepublic void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException { final int maxDoc = state.segmentInfo.maxDoc(); //段的最大文档数 assert pending.size() == maxDoc; final int valueCount = hash.size(); //哈希桶内放置的不同的值的数量 final PackedLongValues ords = pending.build(); //返回ords，其实就是termID进入addValue的顺序 //哈希桶对内部的值进行排序，如果感兴趣可以看看内部的排序算法：内省排序（Introsort），返回的sortedValues是按照termID数组，排序依据是termID的值 final int[] sortedValues = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator()); final int[] ordMap = new int[valueCount]; //通过下面的循环，ordMap获得termID具体排第几的排序map for(int ord=0;ord&lt;valueCount;ord++) { ordMap[sortedValues[ord]] = ord; } //将两个迭代器交给真正的写者，这两个迭代器便是完成ord标签的关键 dvConsumer.addSortedField(fieldInfo, // ord -&gt; value new Iterable&lt;BytesRef&gt;() { @Override public Iterator&lt;BytesRef&gt; iterator() { return new ValuesIterator(sortedValues, valueCount, hash); } }, // doc -&gt; ord new Iterable&lt;Number&gt;() { @Override public Iterator&lt;Number&gt; iterator() { return new OrdsIterator(ordMap, maxDoc, ords); } });} 调用ValuesIterator迭代器next方法将依次返回由低到高的BytesRef 123456789101112131415161718192021222324252627282930private static class ValuesIterator implements Iterator&lt;BytesRef&gt; { final int sortedValues[]; final BytesRefHash hash; final BytesRef scratch = new BytesRef(); final int valueCount; int ordUpto; ValuesIterator(int sortedValues[], int valueCount, BytesRefHash hash) { this.sortedValues = sortedValues; this.valueCount = valueCount; this.hash = hash; } @Override public boolean hasNext() { return ordUpto &lt; valueCount; } @Override public BytesRef next() { if (!hasNext()) { throw new NoSuchElementException(); } hash.get(sortedValues[ordUpto], scratch); //从哈希桶中取出BytesRef ordUpto++; return scratch; } @Override public void remove() { throw new UnsupportedOperationException(); }} OrdsIterator迭代器，将返回依次加入的termID对应的ord 12345678910111213141516171819202122232425262728293031private static class OrdsIterator implements Iterator&lt;Number&gt; { final PackedLongValues.Iterator iter; final int ordMap[]; final int maxDoc; int docUpto; OrdsIterator(int ordMap[], int maxDoc, PackedLongValues ords) { this.ordMap = ordMap; this.maxDoc = maxDoc; assert ords.size() == maxDoc; this.iter = ords.iterator(); } @Override public boolean hasNext() { return docUpto &lt; maxDoc; } @Override public Number next() { if (!hasNext()) { throw new NoSuchElementException(); } int ord = (int) iter.next(); docUpto++; return ord == -1 ? ord : ordMap[ord]; } @Override public void remove() { throw new UnsupportedOperationException(); }} 由源码可见，ord只参考加入此segement的term值，如果需要修改为全局的标签，而忽略排序因素的话，我们需要构造一个单例，这个对象为每个域维护一个map，当有term加入这个addOneValue函数，我们先询问map有没有这个key，如果有就返回value(表示key出现的先后顺序)，没有就加入到map，将最大的value加一当作它的value。在进行flush时通过这个全局对象来生成那两个迭代器，由此便可以得到一个Doc–&gt;Term的正向全局标签索引（标签值只对应与Term值的出现顺序，排序呢能力丢失），但是这个却可以是用单个目录进行多段存储（segment），同时提供标签来进行Distinct与Group By。 以上。","link":"/2017/03/12/2017/如何修改Lucene索引，加速对数据的Distinct、Group By操作/"},{"title":"怎样去最大化Elasticsearch索引性能(第一部分)","text":"本文是How to Maximize Elasticsearch Indexing Performance (Part 1)的翻译版本 这是关于调优 Elasticsearch 索引三部分的第一部分。 这一系列的重点是调优Elasticsearch以达到最大的索引吞吐量并减少监控和管理负荷。 首先，假设你已经开始使用Elasticsearch，创建索引，没有引入框架来填充JSON文档。Elasticsearch会迭代索引JSON文档中的每一个域，判断出它的域，创建对应映射。这听起来很理想，但是Elasticsearch的映射（mapping）不一定总是正确的。如果为域选择了错误的类型，将会出现索引错误。 不用次索引作为模型存储，Elasticsearch就是这样。我们不需要事先定义数据结构，它会从你的文档结构推导并且依此决定去如何创建索引。在这方面，Elasticsearch索引数据使用了隐式模式，而不是显示模式。 因此，索引的决策是非常重要的，在你搜索你的数据的时候它有很大的影响。如果它是字符串域（string field），是否应该标记化和规范化，如果是，怎样进行？如果是数值域（numeric field），需要怎样的精度？里面还有很多类型像日期，空间形状，和父子关系域，它们需要特别注意。 如果不进行分析，存储，甚至发送给Elasticsearch的数据都不需要回应搜索请求。特别地，仔细检查不是由自己定义的映射（mapping）内容（例如，Logstash就会为你自动生成映射）。我们打算去索引大量的数据进Elasticsearch吗？或者我们已经试着这么做了，但是结果吞吐量非常低？如果你搜索需要，在定义我们的索引映射过程中这里有许多优化技术。这篇教程收集了一些列技巧和想法去增加Elasticsearch索引的吞吐量。 自定义域映射对于分析域（ analyzed fields）, 可以使用满足你域需要的最简单的分析器。如果没问题你甚至可以设置为not_analyzed？ 域类型默认是字符串的，被认为是包含全文的。也就是说它们的值在索引前先要通过分析器，对于域的全文查询查询字符串也将在搜索前通过分析器的处理。 对于字符串域有两个非常重要的映射参数是index和analyzer。 index: 这个索引参数控制如何将字符串索引。它有三个值可供选择。 analyzed: 先分析字符串再索引它。换句话说索引的域是可以全文检索的。 not_analyzed: 索引的域是可以被搜索的，但是索引的值是明确的。并不会对它进行分析。 no: 不对这个域索引。这个域不能被搜索。 字符串域默认的索引参数是 analyzed。如果我想映射域作为一个确切的值，我们需要设置为 not_analyzed: 123456{ &quot;tag&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; }} 另外一些简单的类型（例如long, double, date等）也同样接收索引参数，但是对应的值只能是no和not_analyzed，因为他们的值是不用分析的。 analyzer: 用于分析字符串域, 使用分析器参数来指定搜索和索引时使用哪个分析器。默认，Elasticsearch使用standard 分析器,但是你也可以改用一个内建的其他分析器，例如 whitespace，simple 或者 english。 123456{ &quot;tweet&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; }} 禁用 _source 域_source 域包含索引时原始的JSON文档串。_source域它是不会被索引的（因此也是不可以被搜索），但是它是被存储的，在执行例如get或者search请求的时候它可以被返回。 虽然它很方便，但是source导致了索引的额外开销。因为这个原因，可以如下这样禁用： 123456789curl -XPUT ‘http://localhost:9200/index_name/’ -d &apos;{ &quot;mappings&quot;: { &quot;tweet&quot;: { &quot;_source&quot;: { &quot;enabled&quot;: false } } }}&apos; 用户经常没怎么思考后果就禁用了_source域，然后就后悔了。如果_source域是不可用的，有几个特性就不能支持： update, update_by_query, 和 reindex APIs. 即时 高亮. 不能通过一个Elasticsearch索引去重建另一个索引，不能改变映射或者分析设置，或者更新索引到一个新的主版本。 不能通过观察索引时的原始文档去调试查询或者聚合接口。 不具有自动修复索引的能力。 引入和排除_source的域如果你使用_source域, 你设置任何其他域为_stored也不会附加存储。如果你不使用 _source 域,你就必须设置_stored你要存储的域。 注，使用_source可以带来使用更新API的能力。 一个专家级的特性就是剪裁_source域，在文档已经索引后，存储前。移除_source中的域有点类似于禁用_source域，尤其是无法从一个Elasticsearch索引重建另一个索引。 引入、排除参数，也可以接受占位符，可以如下使用： 12345678910111213141516curl -XPUT ‘http://localhost:9200/logs’ -d ‘{ &quot;mappings&quot;: { &quot;event&quot;: { &quot;_source&quot;: { &quot;includes&quot;: [ &quot;*.count&quot;, &quot;meta.*&quot; ], &quot;excludes&quot;: [ &quot;meta.description&quot;, &quot;meta.attributes.*&quot; ] } } }}’ 这些域(1,2,3,4)将被从_source域里移除。 1234567891011121314curl -XPUT ‘http://localhost:9200/logs/event/1’ -d ‘{ &quot;requests&quot;: { &quot;count&quot;: 10, &quot;foo&quot;: &quot;bar&quot; //1 }, &quot;meta&quot;: { &quot;name&quot;: &quot;Some metric&quot;, &quot;description&quot;: &quot;Some metric description&quot;, //2 &quot;attributes&quot;: { &quot;foo&quot;: &quot;one&quot;, //3 &quot;baz&quot;: &quot;two&quot; //4 } }}’ 我们可以搜索 meta.attributes.foo 域, 即使它没有存储在_source域里。 1234567curl -XGET ‘http://localhost:9200/logs/event/_search’ -d ’{ &quot;query&quot;: { &quot;match&quot;: { &quot;meta.attributes.foo&quot;: &quot;one&quot; //searchable field } }}’ 禁用 _all 域_all这个域是一个特殊的，拥有所有域的，使用空格作分隔符将所有字段拼接在一起的大字符串，它可以用于分析和索引，但是不存储。这就意味着他可以被搜索，但是不可以检索。 _all域允许你搜索文档中的值，在你并不知道哪个域包含这个值的时候。这在开始一个新的数据集的时候是非常有用的。举例： 123456789101112curl -XPUT ‘http://localhost:9200/my_index/user/1’ -d ’{ &quot;first_name&quot;: &quot;Will&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;date_of_birth&quot;: &quot;1975-10-25&quot;}’curl -XGET ‘http://localhost:9200/my_index/_search’ -d ’{ &quot;query&quot;: { &quot;match&quot;: { &quot;_all&quot;: &quot;will smith 1975&quot; } }}’ _all完全可以禁用通过在setting中把enabled设为false： 12345678910111213curl -XPUT ‘http://localhost:9200/my_index’ -d ‘{ &quot;mappings&quot;: { &quot;type_1&quot;: { &quot;properties&quot;: {...} }, &quot;type_2&quot;: { &quot;_all&quot;: { &quot;enabled&quot;: false }, &quot;properties&quot;: {...} } }}’ 如果_all域是禁用的，使用URI请求将不可以查询query_string 和 simple_query_string。我们可以配置请求使用默认index.query.default_field配置 1234567891011121314151617curl -XPUT ‘http://localhost:9200/my_index’ -d ‘{ &quot;mappings&quot;: { &quot;my_type&quot;: { &quot;_all&quot;: { &quot;enabled&quot;: false }, &quot;properties&quot;: { &quot;content&quot;: { &quot;type&quot;: &quot;text&quot; } } } }, &quot;settings&quot;: { &quot;index.query.default_field&quot;: &quot;content&quot; }}’ 禁用分析域的 NormsNorms 用于存储各种各样的规范（数值表示域的相对长度和索引时boost设置），它稍后用与在查询文档时计算得分依此排序。 虽然是使用于计算得分，norms还是要使用非常多的内存（甚至是要记录每个域中一个字节一个字节的顺序，即便这个域在这个文档中并不存在）。 如果你并不需要在这个字段上计算得分，你就可以在这个域上禁用norms。仅用于过滤或者聚合的字段就特别适合如此。 使用PUT mapping API 禁用Norms，如下： 12345678910curl -XPUT ‘http://localhost:9200/my_index/_mapping/my_type’ -d ‘{ &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;norms&quot;: { &quot;enabled&quot;: false } } }}’ Norms并不会立即移除，它将在继续索引的时候，在老的段合并为新段的过程中移除。在一个已经移除norms的域上计算分数都可能的到不一致的结果，因为这些域不在有norms，而其他域依然保持有norms。 当下默认的index_options参数你需要存储词项的频率和位置么，默认情况是会这么做的，或者你也可以减少它，可能你只需要文档号。设置 index_options为你真正需要的值，这个参数是影响字符串索引的核心。 index_options 参数用于控制增加到倒排索引的信息，为了搜索和高亮。它可以接受如下设置： docs: 只索引文档号。可以用于回答词项是否存在于文档中的这个域。 freqs: 文档号和词频都会被存储. 词项频率越高积分越高。 positions: 文档号，词项，还有词的位置被索引。位置可以用于模糊或者短语查询。 offsets: 文档号，词项，词的位置，和开始到结束的字符偏移（词项映射到原来的字符串）被索引。 偏移提供postings highlighter。 分析字符串域默认是会使用positions，其他域默认使用docs。 123456789101112131415curl -XPUT ‘http://localhost:9200/my_index’ -d ‘{ &quot;mappings&quot;: { &quot;my_type&quot;: { &quot;properties&quot;: { &quot;text&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;index_options&quot;: &quot;offsets&quot; } } } }}’curl -XPUT ‘http://localhost:9200/my_index/my_type/1’ -d ‘{ &quot;text&quot;: &quot;Quick brown fox&quot;}’ 文本域默认可以使用postings highlighter，索引参数是offsets。 123456789101112curl -XGET ‘http://localhost:9200/my_index/_search’ -d ‘{ &quot;query&quot;: { &quot;match&quot;: { &quot;text&quot;: &quot;brown fox&quot; } }, &quot;highlight&quot;: { &quot;fields&quot;: { &quot;text&quot;: {} } }}’ 使用自动ID功能如果你并没有为每篇原始文档设置ID，那可以使用Elasticsearch自动生成ID的功能。它可以避免查找version因为自动产生的ID是唯一的。 如果使用你自己的ID，试着挑选一个对Lucene友好的ID。例如包含用零填充的IDS,UUID-1, 和 nanotime; 这些ID一致，顺序模式使得非常好进行压缩。相反，以UUID-4作为ID，基本上是随机的，压缩差，Lucene会变慢。 继续第二部分 “How to Maximize Elasticsearch Indexing Performance.’’","link":"/2017/07/23/2017/怎样去最大化Elasticsearch索引性能(第一部分)/"},{"title":"构造自定义领域语言（二）","text":"上星期比较忙哇，鸽了，这个星期开始补，进展缓慢那补什么呢，翻译一篇官方的词法规则文档吧。待我理解完了AST和翻译器部分开始撸代码┑(￣。。￣)┍ 词法规则（Lexer Rules）词法的语法由词法规则组成，从而分成不同的模式。词法模式允许我们将单个词法语法分成多个子规则。词法分析器只能从当前模式中返回匹配的tokens。 词法规则的的定义或多或少遵循了解析器的语法，但是词法规则不能有参数、返回值和局部变量。词法规则名称必须以大写字母开头，用以区别语法解析器的规则名称：12/** Optional document comment */TokenName : alternative1 | ... | alternativeN ; 你也可以定义不是词法tokens的规则，而是帮助识别tokens片段的规则，这些规则对于语法解析器是不可见的： 12fragmentHelperTokenRule : alternative1 | ... | alternativeN ; 例如，DIGIT 是一个很常见的规则： 12INT : DIGIT+ ; // references the DIGIT helper rulefragment DIGIT : [0-9] ; // not a token by itself 词法模式（Lexical Modes）词法模式允许你通过上下文对词法规则进行分组。他就像有多个子运算，但是对于特定上线文只有那么一些。词法分析器只能通过当前模式中的规则进行匹配返回tokens。词法分析器开始是默认模式的，除非指定模式，否则所有规则都是属于默认规则的。在组合的语法中是不能使用词法模式的，只有单词的词法语法定义中可以（请参阅XMLLExer）。12345678rules in default mode...mode MODE1;rules in MODE1...mode MODEN;rules in MODEN... 词法规则元素（Lexer Rule Elements）词法规则允许两种构造方法他们是在语法规则中不可用的：..范围云算法和使用[]扩起字符集。不要将字符集的规则与词法规则混淆。[characters]仅表示characters在词法规则的字符几中，这里是所有词法规则元素的简介： 词法 描述 T 在当前位置允许匹配T，匹配规则名称必须以大写字母开头。 ’literal’ 匹配字符或者字符串文字，例如， ’while’ or ’=’. [char set] 匹配字符集中的一个字符，将x-y解释为范围x和y之间的字符集，包括自身。以下转义字符被解释为单个特殊字符：\\n, \\r, \\b, \\t, and \\f。匹配 ]、\\或者 - 你必须使用转移符 \\。您也可以使用Unicode字符规范：\\uXXXX。这里是一些例子：WS : [ \\n\\u000D] -&gt; skip ; // same as [ \\n\\r] ID : [a-zA-Z] [a-zA-Z0-9]* ; // match usual identifier spec DASHBRACK : [-]]+ ; // match - or ] one or more times ’x’..’y’ 匹配范围x和y之间的任意单个字符。例如，’a’..’z’。 ‘a’..’z’与[a-z]相同。 T 调用语法规则T;一般允许递归，但是左递归是禁止的，T可以是常规tokens或者片段规则（fragment rule）：ID : LETTER (LETTER|’0’..’9’)* ; fragment LETTER : [a-zA-Z\\u0080-\\u00FF_] ; . 点是与任何单个字符匹配的单字符通配符，例如：ESC : ‘\\‘ . ; // match any escaped \\x character {«action»} Lexer操作可以出现在4.2之外的任何位置，而不仅仅出现在最外面用来替代末尾。 词法分析器根据规则中动作的位置来执行动作。 要为具有多个选项执行单个操作，可以将括号括在括号中，然后再执行操作：END : (‘endif’|’end’) {System.out.println(“found an end”);} ;该操作要符合目标语言的语法。 ANTLR将动作的内容逐字复制到生成的代码中;并没有像$ x.y这样的表达式的翻译，因为着是在动作规则中。只有最外层tokens规则中的action被执行。换句话说，如果STRING调用ESC_CHAR和ESC_CHAR有一个动作，当词法分析器在STRING开始匹配时，不执行该动作。 {«p»}? 评价语义谓词«p»。 如果“p”在运行时评估为false，则周围规则变为“不可见”（不可见）。 表达式«p»符合目标语言语法。 虽然语义谓词可以出现在词法规则中的任何地方，但在规则末尾使用它们是最有效的。 一个警告是，语义谓词必须先于词法分析器动作。 请参阅Lexer规则中的谓词。 ~x 匹配不在x中描述的集合中的任何单个字符。Set x可以是单个字符、范围或～（’x’|’y’|’z’）或～[xyz]的子字符集表示。这里有一个规则使用～来匹配任意字符，使用～[\\ r \\ n] ：COMMENT : ‘#’ ~[\\r\\n] ‘\\r’? ‘\\n’ -&gt; skip ; 相对于语法规则，词法规则允许子句在括号和EBNF运算符中：?,,+。COMMENT规则说明了,?的使用规则。使用+对于[0-9]+可以去匹配数字，词法子句也可以使用?作为后缀再EBNF操作中。 递归的词法规则（Recursive Lexer Rules）ANTLR词法规则可以是递归的，不同于大多数词法语法工具。当你想匹配嵌套的tokens，如嵌套的动作块，这是很方便的：{…{…}…}。 123lexer grammar Recur;ACTION : &apos;{&apos; ( ACTION | ~[{}] )* &apos;}&apos; ;WS : [ \\r\\t\\n]+ -&gt; skip ; 冗余字符串字面量（Redundant String Literals）请注意，不要在多个词法分析器规则的右侧指定相同的字符串文字。这样的文字是模糊的，会导致可以匹配多种token类型。ANTLR 会导致语法解析时匹配这类模糊的文字时不可用。对于跨模式的规则也是如此。例如，以下词法语法定义具有相同字符序列的两个标记： 1234lexer grammar L;AND : &apos;&amp;&apos; ;mode STR;MASK : &apos;&amp;&apos; ; 解析器语法规则不能引用文字“＆”，但它可以引用标记的名称： 123456parser grammar P;options { tokenVocab=L; }a : &apos;&amp;&apos; // results in a tool error: no such token AND // no problem MASK // no problem ; 这里构建了一个测试序列： 123$ antlr4 L.g4 # yields L.tokens file needed by tokenVocab option in P.g4$ antlr4 P.g4error(126): P.g4:3:4: cannot create implicit token for string literal &apos;&amp;&apos; in non-combined grammar 词法规则的动作（Lexer Rule Actions）ANTLR词法分析器在匹配词法规则之后创建Token对象。每个对token的请求在Lexer.nextToken中开始，一旦它识别出令牌，它就调用emit函数。emit从词法分析器的当前状态收集的信息为基础以构建token。它访问字段_type，_text，_channel，_tokenStartCharIndex，_tokenStartLine和_tokenStartCharPositionInLine。您可以使用各种setter方法（如setType）设置这些的状态。 例如，如果enumIsKeyword为false，则以下规则将枚举转换为标识符。 1ENUM : &apos;enum&apos; {if (!enumIsKeyword) setType(Identifier);} ; ANTLR在词法操作中没有特殊的$x属性转换（与v3版本不同）对于词法规则，最多可以有一个单独的动作，而不管该规则中有多少备选方案。 词法命令行（Lexer Commands）为了避免将语法绑定到特定的目标语言，ANTLR支持词法命令。与任意嵌入操作不同，这些命令遵循特定语法，并且仅限于几个常用命令。Lexer命令显示在词法分析器规则定义的最外部替换的末尾。像任意动作一样，每个token规则只能有一个。 词法命令由 - &gt;运算符组成，后跟一个或多个可以选择参数的命令名： 12TokenName : «alternative» -&gt; command-nameTokenName : «alternative» -&gt; command-name («identifier or integer») 可选的多个命令，以逗号分隔。这里是有效的命令名称： skip more popMode mode( x ) pushMode( x ) type( x ) channel( x ) 查看图书源代码的用法，一些示例如下所示 skipskip命令告诉词法分析器丢弃那些文本。 1234ID : [a-zA-Z]+ ; // match identifiersINT : [0-9]+ ; // match integersNEWLINE:&apos;\\r&apos;? &apos;\\n&apos; ; // return newlines to parser (is end-statement signal)WS : [ \\t]+ -&gt; skip ; // toss out whitespace mode(), pushMode(), popMode, and more模式命令改变模式堆栈，因此改变词法分析器的模式。 more命令强制词法分析器获得另一个标记，但不抛出当前文本。token类型将是匹配“最终”的规则（即，没有更多或跳过命令的那个）的token类型。 1234567891011// Default &quot;mode&quot;: Everything OUTSIDE of a tagCOMMENT : &apos;&lt;!--&apos; .*? &apos;--&gt;&apos; ;CDATA : &apos;&lt;![CDATA[&apos; .*? &apos;]]&gt;&apos; ;OPEN : &apos;&lt;&apos; -&gt; pushMode(INSIDE) ; ...XMLDeclOpen : &apos;&lt;?xml&apos; S -&gt; pushMode(INSIDE) ;SPECIAL_OPEN: &apos;&lt;?&apos; Name -&gt; more, pushMode(PROC_INSTR) ;// ----------------- Everything INSIDE of a tag ---------------------mode INSIDE;CLOSE : &apos;&gt;&apos; -&gt; popMode ;SPECIAL_CLOSE: &apos;?&gt;&apos; -&gt; popMode ; // close &lt;?xml...?&gt;SLASH_CLOSE : &apos;/&gt;&apos; -&gt; popMode ; 另外检查： 123456lexer grammar Strings;LQUOTE : &apos;&quot;&apos; -&gt; more, mode(STR) ;WS : [ \\r\\t\\n]+ -&gt; skip ;mode STR;STRING : &apos;&quot;&apos; -&gt; mode(DEFAULT_MODE) ; // token we want parser to seeTEXT : . -&gt; more ; // collect more text for string 弹出模式堆栈的底层将导致异常。使用模式切换命令mode更改当前堆栈顶部。一个或者多个more是相同的和位置并没有关系。 type()12345lexer grammar SetType;tokens { STRING }DOUBLE : &apos;&quot;&apos; .*? &apos;&quot;&apos; -&gt; type(STRING) ;SINGLE : &apos;\\&apos;&apos; .*? &apos;\\&apos;&apos; -&gt; type(STRING) ;WS : [ \\r\\t\\n]+ -&gt; skip ; 对于多个type()命令，只有最右边的命令有效果。 channel()123456789101112131415BLOCK_COMMENT : &apos;/*&apos; .*? &apos;*/&apos; -&gt; channel(HIDDEN) ;LINE_COMMENT : &apos;//&apos; ~[\\r\\n]* -&gt; channel(HIDDEN) ;... // ----------// Whitespace//// Characters and character constructs that are of no import// to the parser and are used to make the grammar easier to read// for humans.//WS : [ \\t\\r\\n\\f]+ -&gt; channel(HIDDEN) ; 从4.5开始，您还可以在词法分析器规则上方使用以下结构来定义频道名称（如枚举）： channels { WSCHANNEL, MYHIDDEN }","link":"/2017/01/15/2017/构造自定义领域语言（二）/"},{"title":"Android逆向实践","text":"在讲解 Lucene 数值索引原理之前，我们先回顾一下 Lucene 核心数据结构：倒排索引 倒排索引如示例我们有三篇文档，分为两个字段进行索引，使用各个字段的 Term 来关联文档列表 Posting List，当我们查询 sex:男 And age:18 的时候，[1, 3] ^ [3] 进行交集，就很快人定位到命中的文档 id。 然而倒排索引并没有这个简单，当你索引的 Term 非常多，整个 Term Dictionary 在内存里已经放不下时，不得不使用磁盘存储，使用磁盘存储就需要面对一个问题，如何快速的定位到查询的 Term 对应的 Posting List，于是就有了 Term Index。 Term Index 通过前缀树的形式加载到内存，来加速查找 Terme Dictionary，定位对应的 Posting List。实际上 Lucene 使用了 FST (Finite State Transducers) 的数据结构来代替前缀树，对前缀树进行了很好的压缩，这样内存可以轻松缓存下整个 Term Index。 数值类索引——前世在 Lucene 6.0 之前数值类索引依然需要使用到 FST，数值也会被当做 Term 进行存储，我们简化一下思考数值类如何去利用前缀树进行索引呢？比如我们要索引 [1, 10, 50, 3] 方案一：直接转化为字符串 [“1”, “10”, “50”, “3”] 进行索引存储 这样虽然能马上利用到前缀树索引，但是这样构造出来的前缀树无法进行范围查询，试想一下如果我们查询 &quot;1&quot; &lt; x &lt; &quot;4&quot;，10 也会落在命中的范围内。 方案二：转化为字符串后前面补0，转化为等长字符串再进行索引 [“0001”, “0010”, “0050”, “0003”]。 这样我们查询&quot;1&quot; &lt; x &lt; &quot;4&quot;可以转变为 &quot;0001&quot; &lt; x &lt; &quot;0004&quot;，不会错误的将10查出来，但是带来了另一个问题，存储成本增加，一旦确定字符串长度，更长的数值将无法写入。 Lucene 6.0 之前的解决方案是将数值按步长掩码（shift）转化为多个 Term 进行索引 具体的转化算法，涉及到数值表示的技巧，本文不进行详细的描述，这里按照 16 位一个步长，将一个Long型的 256 转化为4个 Term，分别表示 256 ，[0 -2^16-1]，[0 -2^32-1]，[0 -2^48-1]，写入FST。 256 查询使用同样的算法转为为 Term1，直接命中对应的 Posting List，范围查询 [0 - 65536] ，先进行步长的适配，发现[0 - 65536] 等价于 [0 - 2^16-1] 与 2^16 两个查询，转为Term2 与 Term5 再对 FST 进行查找。 这样既保证了点查与范围查询，同时对数据进行了压缩。 数值类索引——今生从 Lucene 6.0 版本开始，引入针对数值类型，引入了新的数据结构 Block KDB-Tree（Block K-Dimension Balanced Tree），用于优化 Lucene 中范围查询的性能。由于这一索引结构最初用于地理坐标场景，因此被命名为 Point 索引。 我们发现到了 Lucene 6.0 版本数值索引已经抛弃了倒排结构回归使用树的结构，初期 Lucene 数值类索引完全复用字符的索引方式避免引入多种数据结构，但是性能原因还是证明数值类存储还是更适合使用树形结构存储，通过技巧将数值转为了多个范围 Term 方法加大了索引存储空间，范围 Term 边缘的数值，每一个都需要转化成单独的 Term 进行查询效率下降，这些方面在 Block KDB-Tree 上都得到了很好的解决。 新数值类型支持多维数据的写入，这里先看一下一维数据的写入情况。数据写入后保存到内存池里，当 Lucene 触发 flush 动作时，将无序内存进行排序均匀划分叶块，保证每个叶块小于1024个数值，通过叶块再往上构建树干。 对于多维数据的写入，跟一维数据相反，先构建树根，再往下构建树干叶子，第一步选取第一个划分的维度，这里简单粗暴的选择(max - min)最大的维度首先进行 partition（如果使用方差来确定最分散的维度可能会消耗大量CPU时间，降低写入性能），partition 的过程会计算出中值将原来的无序内存划分为左右两块，递归下去选择下一个维度，最后保证叶块数量小于 1024 结束。 Lucene 内的排序算法partition 与 sort 是影响写入性能的关键，Lucene 6.0 使用基数排序与内省排序相结合的方式来代替原内省排序的实现。 内省排序是快排，堆排，插入排序（有的内省算法只包含两种）三者的结合体。我们知道快排是在递归寻找一个主元将数据分为两部分，随着递归的下降，这种划分提供的排序效率越来越低，遇到已经部分有序的序列可能会下降到 O(n^2) 的情况，这时候堆排序就是一个很好的补充，堆排在任何情况下都能保证 O(nlogn) 的时间复杂度，随着排的进行，每个子序列都快接近最后的完成状态，这个时候切换到插入排序，对于几乎已经排好序的序列，插入排序可以达到 O(n) 的时间复杂度。（推荐阅读 http://mindhacks.cn/2008/06/13/why-is-quicksort-so-quick/ ，从信息论的角度分析了快排排序过程中效率的变化 ） 这里先通过一个例子阐述基数排序的原理。按照数值高低位分桶的顺序分为 LSB、MSB 两种基数排序，以 73, 22, 93, 43, 55, 14, 28, 65, 39, 81, 26 MSB举例，如图，先试用十位分桶写入数值，桶内如果有多个数值，则降低分桶位（十位到个位）递归进行下一次分桶。两次分桶后排序结束，只需要从最上层的桶按顺序取出数值即可。 Lucene 6.0 Point 的排序，先通过递归深度和排序区间进行判断，递归深度大于8或者区间小于100的转为内省排序，否则进行MSB的基数排序。由于基数排序相同桶的高位比特是相同的，在内省排序中可以借助这一点减少排序byte位数。 基数排序对于 Partition 也有非常好的加速效果，通过每次分桶后每个桶内数值的数量，就可以快速定位到中值所在桶的位置，只要针对所在桶进行递归，就能快速查找到中值。https://issues.apache.org/jira/browse/LUCENE-7396加入基数排序的效果，提升了13%的写入性能 ###Lucene 索引测试报告 对比项 Point Numbric Store Yes Numbric Store No String Store YES String Store NO 索引文件大小 61.3 M 229.7 M 151.9 M 293.1 M 129.5 M Lucene接口返回的内存消耗 11.74 M 77.97 M 77.97 M 51.34 M 51.34 M 写入耗时 11042 ms 55698 ms 54663 ms 35838 ms 31853 ms 千次平均范围count查询耗时 22 ms 39 ms 37 ms 144 ms 142 ms 参考文章1、Elasitcsearch 底层系列 Lucene 内核解析之Point索引2、Better Query Planning for Range Queries in Elasticsearch3、节省空间：关于 Elasticsearch 中的索引排序，很多人不知道的一项优势4、数学之美番外篇：快排为什么那样快","link":"/2019/07/05/2019/Lucene数值索引原理/"}],"tags":[{"name":"技术","slug":"技术","link":"/tags/技术/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"爬虫","slug":"爬虫","link":"/tags/爬虫/"},{"name":"感","slug":"感","link":"/tags/感/"},{"name":"面经","slug":"面经","link":"/tags/面经/"},{"name":"Groovy","slug":"Groovy","link":"/tags/Groovy/"},{"name":"PHP","slug":"PHP","link":"/tags/PHP/"},{"name":"暴雪哈希","slug":"暴雪哈希","link":"/tags/暴雪哈希/"},{"name":"日志索引","slug":"日志索引","link":"/tags/日志索引/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Lucene","slug":"Lucene","link":"/tags/Lucene/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"前端","slug":"前端","link":"/tags/前端/"},{"name":"svg","slug":"svg","link":"/tags/svg/"},{"name":"ANTLR","slug":"ANTLR","link":"/tags/ANTLR/"},{"name":"领域语言","slug":"领域语言","link":"/tags/领域语言/"},{"name":"逆向","slug":"逆向","link":"/tags/逆向/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"异步","slug":"异步","link":"/tags/异步/"},{"name":"AIO","slug":"AIO","link":"/tags/AIO/"},{"name":"索引","slug":"索引","link":"/tags/索引/"},{"name":"源码","slug":"源码","link":"/tags/源码/"}],"categories":[{"name":"review","slug":"review","link":"/categories/review/"},{"name":"开发","slug":"开发","link":"/categories/开发/"},{"name":"广州","slug":"广州","link":"/categories/广州/"},{"name":"财大","slug":"财大","link":"/categories/财大/"},{"name":"珠海","slug":"珠海","link":"/categories/珠海/"},{"name":"技术","slug":"技术","link":"/categories/技术/"},{"name":"江湖","slug":"江湖","link":"/categories/江湖/"},{"name":"思考","slug":"思考","link":"/categories/思考/"}]}